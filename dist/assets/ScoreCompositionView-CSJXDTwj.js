import{H as d}from"./Header-CnbjAXRj.js";import{A as l,_ as c,a}from"./landmark-down-B0_U1mMh.js";import{d as p,_ as u,c as m,a as i,b as t,r,f as o,F as g,o as v}from"./index-DuQxRGfC.js";const f=p({components:{Header:d,AudioPlayer:l}}),s="data:image/svg+xml,%3csvg%20xmlns='http://www.w3.org/2000/svg'%20width='114'%20height='120'%20fill='none'%20viewBox='0%200%20114%20120'%3e%3cpath%20fill='%23F40B0B'%20d='M59.773%200%2054%205.773l5.773%205.774%205.774-5.774L59.774%200Zm0%20119.547%205.774-5.774L59.774%20108%2054%20113.773l5.773%205.774Zm0-113.774h-1v108h2v-108h-1Z'/%3e%3cpath%20fill='%23F40B0B'%20d='m0%2059.82%205.777%205.769%205.77-5.777-5.777-5.77L0%2059.819Zm5.774-.005v1l54-.034v-1l-.001-1-54%20.034v1Z'/%3e%3c/svg%3e",w={class:"project-section",id:"overview"},y={class:"project-section",id:"tom"},b={class:"project-section",id:"charlotte"},k={class:"project-section",id:"freya"},x={class:"project-section",id:"immersive"},T={class:"instructions"};function I(j,e,S,C,F,B){const h=r("Header"),n=r("AudioPlayer");return v(),m(g,null,[i(h),e[24]||(e[24]=t("img",{src:c,alt:"",class:"page-decoration"},null,-1)),t("main",null,[e[23]||(e[23]=o('<h1 data-v-7099a163>Score Composition Project</h1><h2 data-v-7099a163>Page Contents</h2><nav data-v-7099a163><a href="#" data-v-7099a163>Listen to performance</a><a href="#overview" data-v-7099a163>Overview</a><a href="#tom" data-v-7099a163>Tom&#39;s writing</a><a href="#charlotte" data-v-7099a163>Charlotte&#39;s writing</a><a href="#freya" data-v-7099a163>Freya&#39;s writing</a><a href="#immersive" data-v-7099a163>Freya&#39;s immersive experience (download)</a></nav>',3)),t("section",w,[e[0]||(e[0]=t("h2",null,"Project Overview",-1)),e[1]||(e[1]=t("p",{class:"subheading"},"Listen to text",-1)),e[2]||(e[2]=t("img",{src:a,alt:"",class:"landmark"},null,-1)),i(n,{src:"text-audio/introduction.mp3"}),e[3]||(e[3]=o('<p class="subheading" data-v-7099a163>Or read (1 minute)</p><img src="'+a+'" alt="" class="landmark" data-v-7099a163><p data-v-7099a163> In our group, we decided to explore ideas of oversaturation of information. This came from a discussion surrounding how to reduce our individual climate impact. There are often challenges with knowing how to navigate this, as information can often feel conflicting, leaving people feeling confused about what they should and should not be doing. From this, we decided that it would be interesting to explore ideas of oversaturation in music –– through the use of complex and confusing notation. Through much discussion, we decided to use audio scores, which would provide spoken musical instructions that could communicate complex musical ideas. I composed the musical instructions in the form of text. </p><h3 data-v-7099a163>Creating the oversaturation of information  </h3><p data-v-7099a163> Once we had developed our notation method, we needed to consider ways of making the musical information complex and confusing to interpret. We decided to explore ‘glitching’ the audio, so it would be difficult to interpret into musical instructions. Charlotte was responsible for this, and has written about her process.    </p><p data-v-7099a163> Following the performance, Freya took the stems from the recording and used them to create an interactive audio experience which simulated the concert. This allows listeners to ‘explore’ our performance by moving around the performance space, allowing them to notice how their virtual location in the room changes their listening experience. </p><p data-v-7099a163> This project explores three approaches to music-making, combined into one project which allows each of these approaches to be explored.   </p><div class="spacer" data-v-7099a163><img src="'+s+'" alt="" data-v-7099a163><a href="#" data-v-7099a163>Back to contents</a></div>',8))]),t("section",y,[e[4]||(e[4]=t("h2",null,"Tom: The Composition Process",-1)),e[5]||(e[5]=t("p",{class:"subheading"},"Listen to text",-1)),e[6]||(e[6]=t("img",{src:a,alt:"",class:"landmark"},null,-1)),i(n,{src:"text-audio/introduction.mp3"}),e[7]||(e[7]=o('<p class="subheading" data-v-7099a163>Or read (2 minutes)</p><img src="'+a+'" alt="" class="landmark" data-v-7099a163><p data-v-7099a163> The audio score project consisted of a text score, which was run through a Text-To-Speech (TTS) tool, and the audio from this was played through speakers to Freya and I, whilst Charlotte was responsible for ‘glitching’ the sound whilst we attempted to interpret the instructions. My role within this project, in addition to performing in the concert, was to compose the text scores which would then go on to be used as audio. Through group discussion, we decided to use found objects instead of traditional instruments. The reason for this was to attempt to make the experience unfamiliar for both performers.   </p><p data-v-7099a163> The aim of the piece was for the scores to be ‘glitched’ so they were challenging to interpret for the performers, so within my score, I ensured that the instructions contained multiple instruments and actions, which would become challenging to interpret. This project presented the opportunity to explore ideas of resistance to interpretation, working with text as opposed to traditional notation. </p><p data-v-7099a163> One of the main driving forces in deciding to notate in this way was ensuring that the scores were accessible for everyone in the group. Freya is visually impaired, so we decided to use audio as the means of communication to allow the scores to be interpreted live in the concert. This was a relatively new approach for me as a composer who usually works with written notation. It was essential to me that the score facilitated the communication of complex musical ideas in order to prove that this method could work as an alternative to written notation. When composing this score, I worked with two main musical parameters: rhythm and timbre.    </p><h3 data-v-7099a163>The Score (click through)</h3><h3 data-v-7099a163>Rythm</h3><p data-v-7099a163> I began by considering two rhythmic ideas working simultaneously, as shown in example 1.    </p><p data-v-7099a163> This musical idea can also be written using more traditional notation, as shown in example 2 . </p><p data-v-7099a163> Below is a comparison of the audio playback from the notation software and what was performed in the concert using the audio score.    </p><p data-v-7099a163> As heard in the examples above, both sound similar to each other, despite very different notational approaches. This was the main aim throughout the piece: communicating complex musical ideas through a different medium.    </p><h3 data-v-7099a163>Timbre</h3><p data-v-7099a163> As previously mentioned, this project involved the use of found objects as opposed to traditional instruments. The objects we used were, a homemade rain shaker, metal cutlery and a cello bow. These objects provided interesting ideas for timbral explorations, for example, the shakers were made of corrugated cardboard, so they produced interesting sounds when bowing across them. In addition, part of the shaker was made of acetate, which produced a squeaky pitch when bowed. The metal cutlery also produced interesting timbres when bowed, with loud pitches unexpectedly emerging. These ideas were incorporated into the score, through the use of different bow speeds and pressures.    </p><h3 data-v-7099a163>Predicting the Unpredictable</h3><p data-v-7099a163> One of the challenges when working with unpredictability, from a compositional standpoint is creating unpredictability prior to a performance. Through my MA by research in Composition, I have discussed these ideas considering two approaches to this, chance instability and forced instability. Chance instability involves creating a situation where the possibility for uncertainty is possible, but not guaranteed. An example of this is writing a very quiet high note to be played on a clarinet, which may or may not emerge at that dynamic level. In this situation, there is the possibility that the outcome is very predictable, however nothing is certain. The other approach, chance instability, forces situations on the performer which results in instability being unavoidable. This approach has been taken with the audio scores project. Charlotte’s job as the ‘glitcher’ allowed her alter the legibility of the score by altering how perceivable it was for the performers. She also had the ability to change the order of the score, which is where the forced instability emerges; the performer may be holding just the cutlery when they hear the following instruction:    </p><p data-v-7099a163><i data-v-7099a163>Bow the shaker with slow bows using lots of pressure  </i></p><p data-v-7099a163> What approach should the performer take here? They could put down the cutlery and pick up the bow and shaker, or they could improvise with the situation they are provided with. This score is designed to create an unsettling and unpredictable performance situation, so improvising with their objects is a positive approach to interpretation. This is an example of how forced instability emerges throughout this performance.    </p><div class="spacer" data-v-7099a163><img src="'+s+'" alt="" data-v-7099a163><a href="#" data-v-7099a163>Back to contents</a></div>',18))]),t("section",b,[e[8]||(e[8]=t("h2",null,"Charlotte: The Glitching Project",-1)),e[9]||(e[9]=t("p",{class:"subheading"},"Listen to text",-1)),e[10]||(e[10]=t("img",{src:a,alt:"",class:"landmark"},null,-1)),i(n,{src:"text-audio/introduction.mp3"}),e[11]||(e[11]=o('<p class="subheading" data-v-7099a163>Or read (2 minutes)</p><img src="'+a+'" alt="" class="landmark" data-v-7099a163><h3 data-v-7099a163>Something &#39;political&#39;</h3><p data-v-7099a163> Something we talked about early on in conceiving this collaboration was the want to &#39;do something political&#39;.  </p><p data-v-7099a163> I think an important thing to remember when your aim is to make political art, is that it&#39;s not the same as activism. The point of the latter being that you&#39;re (hopefully) communicating clearly, (hopefully) in pursuit of a clear goal.  </p><p data-v-7099a163> The point of the former is less clear, but I think the point isn&#39;t to know the point. Do you see my point?  </p><h3 data-v-7099a163>Tangles</h3><p data-v-7099a163> We&#39;re chatting about the climate crisis, and at first it&#39;s a lot of science, a lot of data, and a lot of facts. Very straightforward.  </p><p data-v-7099a163> But I don&#39;t feel satisfied by the idea that we just make a work that points out something bad.  </p><p data-v-7099a163> Or it&#39;s that it isn&#39;t artistically motivating. If the point isn&#39;t activism then we can look at it much more sideways.  </p><p data-v-7099a163>I already have this interest in glitch, and Tom in fragility. </p><p data-v-7099a163> Freya tells us a story about a friend who was arrested at a Just Stop Oil action. They don&#39;t talk about it with their family because they get the whole &#39;well you wear nylon / eat meat / drive a petrol car&#39; type of arguments.  </p><p data-v-7099a163> How can you be against x when you are entangled in the system that makes x impossible to avoid?  </p><p data-v-7099a163> It&#39;s a blame system. It makes you feel like there&#39;s nothing you can do that&#39;s right. The more you pull the more complex it gets.  </p><p data-v-7099a163> We talk about creating a performance system that works against itself.  </p><p data-v-7099a163> Something that utilises confusion, complexity, and information saturation to achieve a certain sonic result. It ‘works’ by ‘not working’.  </p><p data-v-7099a163>Strategic error. </p><p data-v-7099a163> The idea: we start with a text score, this is played back for musicians to interpret in real time, and there is an intervening step after the score’s input to glitch/error/obscure the output.  </p><p data-v-7099a163>Score to glitches to musicians.</p><h3 data-v-7099a163>The Tool</h3><p data-v-7099a163> The way this works is that I ran each step in Tom&#39;s text score through a text to speech program. Producing and mp3 of each step in a different voice, with instructions for Tom panned all the way left and instructions for Freya panned all the way right.  </p><p data-v-7099a163> In the concert, we played through stereo speakers, with the output of my glitching instrument being outputted to the audience in the same way Tom and Freya were hearing it. That way, the playback of the score was just as much part of the performance as anything else.  </p><p data-v-7099a163>For each step I could:</p><ul data-v-7099a163><li data-v-7099a163>Start / stop the instructions</li><li data-v-7099a163>Adjust the volume</li><li data-v-7099a163>Timestretch (slow and pitch down, speed and pitch up) </li><li data-v-7099a163> Bitcrush (a method of distorting through compressing audio data)  </li><li data-v-7099a163> Reverse the channels (Tom’s instructions go to Freya, Freya’s go to Tom’s)  </li></ul><p data-v-7099a163> Playing back while manipulating the score with this forms my contribution to the performance.  </p><p data-v-7099a163> Code: <a href=" https://github.com/ckjr3000/text-score-glitcher" data-v-7099a163> https://github.com/ckjr3000/text-score-glitcher</a></p><h3 data-v-7099a163>Reflections</h3><p data-v-7099a163>First, the definition of &#39;glitch&#39; I work with is: </p><p data-v-7099a163><i data-v-7099a163>A presence in a system that isn&#39;t supposed to be there </i></p><p data-v-7099a163> Which obviously is immediately challenged when you start to use glitch as an intentional aesthetic element. But we like tangles. Tangles are glitches.  </p><p data-v-7099a163> In this case, my goal was to disrupt the score enough to surprise and challenge Freya and Tom, without tipping over into illegibility. So I chose audio effects that would obscure rather than obliterate the source material.  </p><p data-v-7099a163> In practice, I found starting and stopping instructions during playback, and mixing up the order of playback to yield some of the best results. For example, prompting them to make a change to what they were doing with an instrument they hadn&#39;t been told to pick up yet, or repeating an instruction to go faster or slower until it reaches some ridiculous limit.  </p><p data-v-7099a163> As fun as it was to act as puppet master behind my laptop screen, I believe a second iteration of this performance could be designed. One in which some output from the musicians could serve to glitch MY glitching process. A feedback loop wherein everyone can&#39;t help but to destabilise everyone else just by taking part.  </p><div class="spacer" data-v-7099a163><img src="'+s+'" alt="" data-v-7099a163><a href="#" data-v-7099a163>Back to contents</a></div>',34))]),t("section",k,[e[12]||(e[12]=t("h2",null,"Freya: The Interactive Experience",-1)),e[13]||(e[13]=t("p",{class:"subheading"},"Listen to text",-1)),e[14]||(e[14]=t("img",{src:a,alt:"",class:"landmark"},null,-1)),i(n,{src:"text-audio/introduction.mp3"}),e[15]||(e[15]=o('<p class="subheading" data-v-7099a163>Or read (2 minutes)</p><img src="'+a+'" alt="" class="landmark" data-v-7099a163><p data-v-7099a163> My contribution to the project predominately took place post-performance in the form of an interactive audio experience simulating the concert. As part of my MA in Music Technology by research, I am developing an audio game engine, the Hodr Engine. This assists blind and sighted users in creating audio games using a screen-reader-friendly integrated development environment, and a programming language, HodrScript, which places sounds within a virtual environment to generate interactive stereo soundscapes. The audio games, developed with the Hodr Engine, are interacted through key presses.   </p><p data-v-7099a163> For background, I am researching accessibility in game development and play after discovering many existing game engines are inaccessible when operated with a screen reader. A screen reader is a tool for blind and visually impaired users that translates the content on a screen into an audio output of a synthesised voice or Braille. Therefore, I am developing a screen reader adaptive game engine, the Hodr Engine. This platform allows blind and sighted users to create audio games, through a blind and visually impaired inclusive integrated development environment (IDE), implementing features within the IDE, such as accurately labelled buttons, alt text to describe visuals and a text-based system for writing code, often translating clearer than a visual interface.   </p><p data-v-7099a163>  The CeReNeM journal performance was an exciting opportunity to test the creation of an interactive soundscape using stem files recorded live during the performance. This contrasts with my previous work within the engine, which used separately recorded audio files. I aimed to create a realistic simulation in which the user could experience the concert as a listener who could navigate among the performers, hearing from any perspective in the space.   </p><p data-v-7099a163> In the virtual space, the environment is structured on an X-Y grid system, where sounds can be placed at specific coordinates using HodrScript. The grid can range in size from as small as 1 by 1 to as large as 200 by 200. Each step taken by the player is roughly equivalent to the width of 1.5 grid squares.  </p><p data-v-7099a163> Of course, this grid-based explanation is simply to help you, the reader, imagine the layout of the environment from a top-down perspective. When exploring the virtual world in practice, the screen does not display the player moving around a visible grid. Instead, the user experiences the space through sound, listening from their own point of view as if walking through the environment. Visually, they are presented with a static image, while the audio creates the world around them. During the performance, microphones recorded each participant, and the resulting stem files were imported within the Hodr Engine and plotted on a map with coordinates, simulating their positions on stage at Phipps Hall in the Richard Steinitz Building. While the real hall is larger than the virtual environment, the user’s experience is focused on the stage itself within the experience to prevent getting lost in quieter areas of the room. The coordinates were adjusted to scale the virtual stage accurately within the engine’s limitations.   </p><p data-v-7099a163> However, I encountered several challenges in creating the virtual environment. The stem files were recorded simultaneously, so microphones picked up echoes of other sounds, which caused overlapping. This was pronounced in the Audio Scores performance, where the TTS voices, louder than Tom and my contributions, were duplicated across multiple stem files, making navigating difficult. The current Hodr Engine version lacks built-in reverberation and occlusion effects, meaning such effects must be pre-applied to the stem files. I edited each stem file in Logic Pro, a digital audio workstation, to replicate hall reverberation and applied binaural placement for height. Most performers sat on the stage floor, so their sounds were placed lower in the soundscape, while the TTS voices emitted from speakers positioned higher up. Some effects like reverberation were effective, but distinguishing whether a sound was in front or behind the listener remained challenging due to the absence of occlusion.   </p><p data-v-7099a163> Overall, the CeReNeM Audio Experience was a valuable introduction to using the Hodr Engine for creating live-recorded virtual environments. One highlight was Charlotte’s improvisation, which featured static-like sounds bouncing between two speakers, creating a back-and-forth effect for the listener situated between them.   </p><div class="spacer" data-v-7099a163><img src="'+s+'" alt="" data-v-7099a163><a href="#" data-v-7099a163>Back to contents</a></div>',10))]),t("section",x,[e[21]||(e[21]=o('<h2 data-v-7099a163>Freya&#39;s Immersive Experience</h2><a href="#" data-v-7099a163>Download</a><div class="instructions" data-v-7099a163><h3 data-v-7099a163>Navigational Instructions</h3><p class="subheading" data-v-7099a163>Listen:</p><p class="subheading" data-v-7099a163>Or read:</p><p data-v-7099a163> Upon launching the CeReNeM Interactive Audio Experience application, the user is greeted by one of the TTS voices, heard in the Audio Scores project, providing instructions for navigating the audio experience. These instructions are detailed below. </p><p data-v-7099a163>Welcome to the CeReNeM Journal’s interactive audio experience.  </p><p data-v-7099a163> You’re currently in the menu screen. Use the up and down arrow keys to navigate through the menu options: and with the space bar,  select Start to begin the experience, Continue to resume from where you left off, or Cancel to exit.   </p><p data-v-7099a163> By selecting Start, you will hear a recording of a contemporary music performance taken at the University of Huddersfield for the CeReNeM Journal. This performance features two improvisations.    </p><p data-v-7099a163> The first improvisation, titled Audio Scores, involves two performers responding to a third member’s text-to-speech commands as they improvise with a rain shaker, spoon, and cello bow. In the second piece, five musicians improvise with a blend of acoustic and electronic instruments.   </p><p data-v-7099a163> This audio experience invites you to explore the original stem files of the performance in a spatial audio environment. During the experience, navigate using the up and down arrow keys to move forward and back, and the left and right arrow keys to turn. Press Q anytime to exit.  </p></div>',3)),t("div",T,[e[16]||(e[16]=t("h3",null,"Installtion Instructions",-1)),e[17]||(e[17]=t("p",{class:"subheading"},"Listen to text",-1)),e[18]||(e[18]=t("img",{src:a,alt:"",class:"landmark"},null,-1)),i(n,{src:"text-audio/introduction.mp3"}),e[19]||(e[19]=t("p",{class:"subheading"},"Or read (1 minute)",-1)),e[20]||(e[20]=t("img",{src:a,alt:"",class:"landmark"},null,-1))]),e[22]||(e[22]=t("div",{class:"spacer"},[t("img",{src:s,alt:""}),t("a",{href:"#"},"Back to contents")],-1))])])],64)}const E=u(f,[["render",I],["__scopeId","data-v-7099a163"]]);export{E as default};
