<template>
  <Header />
  <img src="../assets/vectors/page-accent.svg" alt="" class="page-decoration" />
  <main>
    <h1>Score Composition Project</h1>
    <h2>Page Contents</h2>
    <nav class="page-nav">
      <a href="#overview">Overview</a>
      <a href="#">Performance</a>
      <a href="#tom">Tom's writing</a>
      <a href="#charlotte">Charlotte's writing</a>
      <a href="#freya">Freya's writing</a>
      <a href="#immersive">Freya's immersive experience (download)</a>
    </nav>

    <section class="project-section" id="overview">
      <h2>Project Overview</h2>
      <p class="subheading">Listen to text</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <AudioPlayer src="text-audio/introduction.mp3" />
      <p class="subheading">Or read (1 minute)</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <p>
        In our group, we decided to explore ideas of oversaturation of
        information. This came from a discussion surrounding how to reduce our
        individual climate impact. There are often challenges with knowing how
        to navigate this, as information can often feel conflicting, leaving
        people feeling confused about what they should and should not be doing.
        From this, we decided that it would be interesting to explore ideas of
        oversaturation in music –– through the use of complex and confusing
        notation. Through much discussion, we decided to use audio scores, which
        would provide spoken musical instructions that could communicate complex
        musical ideas. I composed the musical instructions in the form of text.
      </p>
      <h3>Creating the oversaturation of information  </h3>
      <p>
        Once we had developed our notation method, we needed to consider ways of
        making the musical information complex and confusing to interpret. We
        decided to explore ‘glitching’ the audio, so it would be difficult to
        interpret into musical instructions. Charlotte was responsible for this,
        and has written about her process.   
      </p>
      <p>
        Following the performance, Freya took the stems from the recording and
        used them to create an interactive audio experience which simulated the
        concert. This allows listeners to ‘explore’ our performance by moving
        around the performance space, allowing them to notice how their virtual
        location in the room changes their listening experience.
      </p>
      <p>
        This project explores three approaches to music-making, combined into
        one project which allows each of these approaches to be explored.  
      </p>
      <div class="spacer">
        <img src="../assets/vectors/cross-roads-spacer.svg" alt="" />
        <a href="#">Back to contents</a>
      </div>
    </section>

    <section class="project-section" id="tom">
      <h2>Tom: The Composition Process</h2>
      <p class="subheading">Listen to text</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <AudioPlayer src="text-audio/introduction.mp3" />
      <p class="subheading">Or read (2 minutes)</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <p>
        The audio score project consisted of a text score, which was run through
        a Text-To-Speech (TTS) tool, and the audio from this was played through
        speakers to Freya and I, whilst Charlotte was responsible for
        ‘glitching’ the sound whilst we attempted to interpret the instructions.
        My role within this project, in addition to performing in the concert,
        was to compose the text scores which would then go on to be used as
        audio. Through group discussion, we decided to use found objects instead
        of traditional instruments. The reason for this was to attempt to make
        the experience unfamiliar for both performers.  
      </p>
      <p>
        The aim of the piece was for the scores to be ‘glitched’ so they were
        challenging to interpret for the performers, so within my score, I
        ensured that the instructions contained multiple instruments and
        actions, which would become challenging to interpret. This project
        presented the opportunity to explore ideas of resistance to
        interpretation, working with text as opposed to traditional notation.
      </p>
      <p>
        One of the main driving forces in deciding to notate in this way was
        ensuring that the scores were accessible for everyone in the group.
        Freya is visually impaired, so we decided to use audio as the means of
        communication to allow the scores to be interpreted live in the concert.
        This was a relatively new approach for me as a composer who usually
        works with written notation. It was essential to me that the score
        facilitated the communication of complex musical ideas in order to prove
        that this method could work as an alternative to written notation. When
        composing this score, I worked with two main musical parameters: rhythm
        and timbre.   
      </p>
      <h3>The Score (click through)</h3>
      <!-- slider component -->
      <!-- <figure>tbc</figure>
    <figcaption>Example 1</figcaption>

    <figure>tbc</figure>
    <figcaption>Example 2</figcaption> -->

      <h3>Rythm</h3>
      <p>
        I began by considering two rhythmic ideas working simultaneously, as
        shown in example 1.   
      </p>
      <p>
        This musical idea can also be written using more traditional notation,
        as shown in example 2 .
      </p>
      <p>
        Below is a comparison of the audio playback from the notation software
        and what was performed in the concert using the audio score.   
      </p>
      <p>
        As heard in the examples above, both sound similar to each other,
        despite very different notational approaches. This was the main aim
        throughout the piece: communicating complex musical ideas through a
        different medium.   
      </p>
      <!-- audio component -->
      <!-- audio component -->
      <h3>Timbre</h3>
      <p>
        As previously mentioned, this project involved the use of found objects
        as opposed to traditional instruments. The objects we used were, a
        homemade rain shaker, metal cutlery and a cello bow. These objects
        provided interesting ideas for timbral explorations, for example, the
        shakers were made of corrugated cardboard, so they produced interesting
        sounds when bowing across them. In addition, part of the shaker was made
        of acetate, which produced a squeaky pitch when bowed. The metal cutlery
        also produced interesting timbres when bowed, with loud pitches
        unexpectedly emerging. These ideas were incorporated into the score,
        through the use of different bow speeds and pressures.   
      </p>
      <h3>Predicting the Unpredictable</h3>
      <p>
        One of the challenges when working with unpredictability, from a
        compositional standpoint is creating unpredictability prior to a
        performance. Through my MA by research in Composition, I have discussed
        these ideas considering two approaches to this, chance instability and
        forced instability. Chance instability involves creating a situation
        where the possibility for uncertainty is possible, but not guaranteed.
        An example of this is writing a very quiet high note to be played on a
        clarinet, which may or may not emerge at that dynamic level. In this
        situation, there is the possibility that the outcome is very
        predictable, however nothing is certain. The other approach, chance
        instability, forces situations on the performer which results in
        instability being unavoidable. This approach has been taken with the
        audio scores project. Charlotte’s job as the ‘glitcher’ allowed her
        alter the legibility of the score by altering how perceivable it was for
        the performers. She also had the ability to change the order of the
        score, which is where the forced instability emerges; the performer may
        be holding just the cutlery when they hear the following instruction:   
      </p>
      <p><i>Bow the shaker with slow bows using lots of pressure  </i></p>
      <p>
        What approach should the performer take here? They could put down the
        cutlery and pick up the bow and shaker, or they could improvise with the
        situation they are provided with. This score is designed to create an
        unsettling and unpredictable performance situation, so improvising with
        their objects is a positive approach to interpretation. This is an
        example of how forced instability emerges throughout this
        performance.   
      </p>
      <div class="spacer">
        <img src="../assets/vectors/cross-roads-spacer.svg" alt="" />
        <a href="#">Back to contents</a>
      </div>
    </section>

    <section class="project-section" id="charlotte">
      <h2>Charlotte: The Glitching Project</h2>
      <p class="subheading">Listen to text</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <AudioPlayer src="text-audio/introduction.mp3" />
      <p class="subheading">Or read (2 minutes)</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <h3>Something 'political'</h3>
      <p>
        Something we talked about early on in conceiving this collaboration was
        the want to 'do something political'. 
      </p>
      <p>
        I think an important thing to remember when your aim is to make
        political art, is that it's not the same as activism. The point of the
        latter being that you're (hopefully) communicating clearly, (hopefully)
        in pursuit of a clear goal. 
      </p>
      <p>
        The point of the former is less clear, but I think the point isn't to
        know the point. Do you see my point? 
      </p>
      <h3>Tangles</h3>
      <p>
        We're chatting about the climate crisis, and at first it's a lot of
        science, a lot of data, and a lot of facts. Very straightforward. 
      </p>
      <p>
        But I don't feel satisfied by the idea that we just make a work that
        points out something bad. 
      </p>
      <p>
        Or it's that it isn't artistically motivating. If the point isn't
        activism then we can look at it much more sideways. 
      </p>
      <p>I already have this interest in glitch, and Tom in fragility. </p>
      <p>
        Freya tells us a story about a friend who was arrested at a Just Stop
        Oil action. They don't talk about it with their family because they get
        the whole 'well you wear nylon / eat meat / drive a petrol car' type of
        arguments. 
      </p>
      <p>
        How can you be against x when you are entangled in the system that makes
        x impossible to avoid? 
      </p>
      <p>
        It's a blame system. It makes you feel like there's nothing you can do
        that's right. The more you pull the more complex it gets. 
      </p>
      <p>
        We talk about creating a performance system that works against itself. 
      </p>
      <p>
        Something that utilises confusion, complexity, and information
        saturation to achieve a certain sonic result. It ‘works’ by ‘not
        working’. 
      </p>
      <p>Strategic error. </p>
      <p>
        The idea: we start with a text score, this is played back for musicians
        to interpret in real time, and there is an intervening step after the
        score’s input to glitch/error/obscure the output. 
      </p>
      <p>Score to glitches to musicians.</p>
      <h3>The Tool</h3>
      <!-- some kind of embed of the tool -->
      <!-- <figure></figure>
    <figcaption>My instrument</figcaption> -->
      <p>
        The way this works is that I ran each step in Tom's text score through a
        text to speech program. Producing and mp3 of each step in a different
        voice, with instructions for Tom panned all the way left and
        instructions for Freya panned all the way right. 
      </p>
      <p>
        In the concert, we played through stereo speakers, with the output of my
        glitching instrument being outputted to the audience in the same way Tom
        and Freya were hearing it. That way, the playback of the score was just
        as much part of the performance as anything else. 
      </p>
      <p>For each step I could:</p>
      <ul>
        <li>Start / stop the instructions</li>
        <li>Adjust the volume</li>
        <li>Timestretch (slow and pitch down, speed and pitch up) </li>
        <li>
          Bitcrush (a method of distorting through compressing audio data) 
        </li>
        <li>
          Reverse the channels (Tom’s instructions go to Freya, Freya’s go to
          Tom’s) 
        </li>
      </ul>
      <p>
        Playing back while manipulating the score with this forms my
        contribution to the performance. 
      </p>
      <p>
        Code:
        <a href=" https://github.com/ckjr3000/text-score-glitcher">
          https://github.com/ckjr3000/text-score-glitcher</a
        >
      </p>
      <h3>Reflections</h3>
      <p>First, the definition of 'glitch' I work with is: </p>
      <p><i>A presence in a system that isn't supposed to be there </i></p>
      <p>
        Which obviously is immediately challenged when you start to use glitch
        as an intentional aesthetic element. But we like tangles. Tangles are
        glitches. 
      </p>
      <p>
        In this case, my goal was to disrupt the score enough to surprise and
        challenge Freya and Tom, without tipping over into illegibility. So I
        chose audio effects that would obscure rather than obliterate the source
        material. 
      </p>
      <p>
        In practice, I found starting and stopping instructions during playback,
        and mixing up the order of playback to yield some of the best results.
        For example, prompting them to make a change to what they were doing
        with an instrument they hadn't been told to pick up yet, or repeating an
        instruction to go faster or slower until it reaches some ridiculous
        limit. 
      </p>
      <p>
        As fun as it was to act as puppet master behind my laptop screen, I
        believe a second iteration of this performance could be designed. One in
        which some output from the musicians could serve to glitch MY glitching
        process. A feedback loop wherein everyone can't help but to destabilise
        everyone else just by taking part. 
      </p>
      <div class="spacer">
        <img src="../assets/vectors/cross-roads-spacer.svg" alt="" />
        <a href="#">Back to contents</a>
      </div>
    </section>

    <section class="project-section" id="freya">
      <h2>Freya: The Interactive Experience</h2>
      <p class="subheading">Listen to text</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <AudioPlayer src="text-audio/introduction.mp3" />
      <p class="subheading">Or read (2 minutes)</p>
      <img src="../assets/vectors/landmark-down.svg" alt="" class="landmark" />
      <p>
        My contribution to the project predominately took place post-performance
        in the form of an interactive audio experience simulating the concert.
        As part of my MA in Music Technology by research, I am developing an
        audio game engine, the Hodr Engine. This assists blind and sighted users
        in creating audio games using a screen-reader-friendly integrated
        development environment, and a programming language, HodrScript, which
        places sounds within a virtual environment to generate interactive
        stereo soundscapes. The audio games, developed with the Hodr Engine, are
        interacted through key presses.  
      </p>
      <p>
        For background, I am researching accessibility in game development and
        play after discovering many existing game engines are inaccessible when
        operated with a screen reader. A screen reader is a tool for blind and
        visually impaired users that translates the content on a screen into an
        audio output of a synthesised voice or Braille. Therefore, I am
        developing a screen reader adaptive game engine, the Hodr Engine. This
        platform allows blind and sighted users to create audio games, through a
        blind and visually impaired inclusive integrated development environment
        (IDE), implementing features within the IDE, such as accurately labelled
        buttons, alt text to describe visuals and a text-based system for
        writing code, often translating clearer than a visual interface.  
      </p>
      <p>
         The CeReNeM journal performance was an exciting opportunity to test the
        creation of an interactive soundscape using stem files recorded live
        during the performance. This contrasts with my previous work within the
        engine, which used separately recorded audio files. I aimed to create a
        realistic simulation in which the user could experience the concert as a
        listener who could navigate among the performers, hearing from any
        perspective in the space.  
      </p>
      <p>
        In the virtual space, the environment is structured on an X-Y grid
        system, where sounds can be placed at specific coordinates using
        HodrScript. The grid can range in size from as small as 1 by 1 to as
        large as 200 by 200. Each step taken by the player is roughly equivalent
        to the width of 1.5 grid squares. 
      </p>
      <p>
        Of course, this grid-based explanation is simply to help you, the
        reader, imagine the layout of the environment from a top-down
        perspective. When exploring the virtual world in practice, the screen
        does not display the player moving around a visible grid. Instead, the
        user experiences the space through sound, listening from their own point
        of view as if walking through the environment. Visually, they are
        presented with a static image, while the audio creates the world around
        them. During the performance, microphones recorded each participant, and
        the resulting stem files were imported within the Hodr Engine and
        plotted on a map with coordinates, simulating their positions on stage
        at Phipps Hall in the Richard Steinitz Building. While the real hall is
        larger than the virtual environment, the user’s experience is focused on
        the stage itself within the experience to prevent getting lost in
        quieter areas of the room. The coordinates were adjusted to scale the
        virtual stage accurately within the engine’s limitations.  
      </p>
      <p>
        However, I encountered several challenges in creating the virtual
        environment. The stem files were recorded simultaneously, so microphones
        picked up echoes of other sounds, which caused overlapping. This was
        pronounced in the Audio Scores performance, where the TTS voices, louder
        than Tom and my contributions, were duplicated across multiple stem
        files, making navigating difficult. The current Hodr Engine version
        lacks built-in reverberation and occlusion effects, meaning such effects
        must be pre-applied to the stem files. I edited each stem file in Logic
        Pro, a digital audio workstation, to replicate hall reverberation and
        applied binaural placement for height. Most performers sat on the stage
        floor, so their sounds were placed lower in the soundscape, while the
        TTS voices emitted from speakers positioned higher up. Some effects like
        reverberation were effective, but distinguishing whether a sound was in
        front or behind the listener remained challenging due to the absence of
        occlusion.  
      </p>
      <p>
        Overall, the CeReNeM Audio Experience was a valuable introduction to
        using the Hodr Engine for creating live-recorded virtual environments.
        One highlight was Charlotte’s improvisation, which featured static-like
        sounds bouncing between two speakers, creating a back-and-forth effect
        for the listener situated between them.  
      </p>
      <!-- code block component -->
      <div class="spacer">
        <img src="../assets/vectors/cross-roads-spacer.svg" alt="" />
        <a href="#">Back to contents</a>
      </div>
    </section>

    <section class="project-section" id="immersive">
      <h2>Freya's Immersive Experience</h2>
      <a href="#">Download</a>

      <div class="instructions">
        <h3>Navigational Instructions</h3>
        <p class="subheading">Listen:</p>
        <!-- audio component -->
        <p class="subheading">Or read:</p>
        <!-- make this collapsable? -->
        <p>
          Upon launching the CeReNeM Interactive Audio Experience application,
          the user is greeted by one of the TTS voices, heard in the Audio
          Scores project, providing instructions for navigating the audio
          experience. These instructions are detailed below.
        </p>
        <p>Welcome to the CeReNeM Journal’s interactive audio experience.  </p>
        <p>
          You’re currently in the menu screen. Use the up and down arrow keys to
          navigate through the menu options: and with the space bar,  select
          Start to begin the experience, Continue to resume from where you left
          off, or Cancel to exit.  
        </p>
        <p>
          By selecting Start, you will hear a recording of a contemporary music
          performance taken at the University of Huddersfield for the CeReNeM
          Journal. This performance features two improvisations.   
        </p>
        <p>
          The first improvisation, titled Audio Scores, involves two performers
          responding to a third member’s text-to-speech commands as they
          improvise with a rain shaker, spoon, and cello bow. In the second
          piece, five musicians improvise with a blend of acoustic and
          electronic instruments.  
        </p>
        <p>
          This audio experience invites you to explore the original stem files
          of the performance in a spatial audio environment. During the
          experience, navigate using the up and down arrow keys to move forward
          and back, and the left and right arrow keys to turn. Press Q anytime
          to exit. 
        </p>
      </div>

      <div class="instructions">
        <h3>Installtion Instructions</h3>
        <p class="subheading">Listen to text</p>
        <img
          src="../assets/vectors/landmark-down.svg"
          alt=""
          class="landmark"
        />
        <AudioPlayer src="text-audio/introduction.mp3" />
        <p class="subheading">Or read (1 minute)</p>
        <img
          src="../assets/vectors/landmark-down.svg"
          alt=""
          class="landmark"
        />
      </div>
      <div class="spacer">
        <img src="../assets/vectors/cross-roads-spacer.svg" alt="" />
        <a href="#">Back to contents</a>
      </div>
    </section>
  </main>
</template>

<script lang="ts">
import { defineComponent } from "vue";

// @ts-ignore
import Header from "@/components/Header.vue";

//@ts-ignore
import AudioPlayer from "@/components/AudioPlayer.vue";

export default defineComponent({
  components: { Header, AudioPlayer },
});
</script>
